{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.stats import multivariate_normal\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "import torch\n",
    "from itertools import repeat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-126bd900e0d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mcovs_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcovs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mX_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mmus_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mcovs_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcovs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/skymul/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \"multiprocessing, you must use the 'spawn' start method\")\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_cuda_getDeviceCount'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             raise AssertionError(\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "n_components = 30\n",
    "X = np.random.rand(100000,2)\n",
    "mus = np.random.rand(n_components,2)\n",
    "covs = np.array([[1,0],[0,0.5]])\n",
    "covs = np.tile(covs,(n_components,1,1)).astype(np.float64)\n",
    "def fun(X_mu_cov):\n",
    "    X,mu,cov = X_mu_cov\n",
    "    distribution = multivariate_normal(mean=mu, cov=cov,allow_singular=True)\n",
    "    distribution.pdf(X)\n",
    "def fun2(X,mu,cov):\n",
    "    distribution = multi_pdf(X,mu, cov)\n",
    "\n",
    "num_workers = os.cpu_count()\n",
    "X_tensor = torch.from_numpy(X)\n",
    "mus_tensor = torch.from_numpy(mus)\n",
    "covs_tensor = torch.from_numpy(covs)\n",
    "\n",
    "X_batch = torch.from_numpy(X).view(1,-1,2).cuda()\n",
    "mus_batch = torch.from_numpy(mus).view(n_components,1,2).cuda()\n",
    "covs_batch = torch.from_numpy(covs).view(n_components,2,2).cuda()\n",
    "\n",
    "def multi_pdf(X,mu,cov,k=2):\n",
    "    return torch.exp(-0.5* ((X-mu)**2 @ torch.linalg.inv(cov)).sum(1))/torch.sqrt((2*np.pi)**k*torch.linalg.det(cov))\n",
    "\n",
    "def batch_multi_pdf(X,mu,cov,k=2):\n",
    "    return torch.exp(-0.5* ((X-mu)**2 @ torch.linalg.inv(cov)).sum(1))/torch.sqrt((2*np.pi)**k*torch.linalg.det(cov))\n",
    "\n",
    "    \n",
    "cpu_normal_time = [];cpu_parallel_time=[];torch_normal_time=[];torch_cuda_time=[];torch_cuda_batch_time = []\n",
    "\n",
    "for loop in range(100):\n",
    "    # CPU normal\n",
    "    start_time = time.time()\n",
    "    for mu, cov in zip(mus,covs):\n",
    "        distribution = multivariate_normal(mean=mu, cov=cov,allow_singular=True)\n",
    "        p1=distribution.pdf(X)\n",
    "    cpu_normal_time.append(time.time() - start_time)\n",
    "\n",
    "#     # CPU parallel\n",
    "#     start_time = time.time()\n",
    "#     with Pool(num_workers) as p:\n",
    "#         p.map(fun,zip(X,mus,covs))\n",
    "#     cpu_parallel_time.append(time.time() - start_time)\n",
    "\n",
    "\n",
    "#     # torch - CPUtensor([7.0437e-108, 1.0520e-209], device='cuda:0', dtype=torch.float64)\n",
    "#     start_time = time.time()\n",
    "#     for mu, cov in zip(mus_tensor,covs_tensor):\n",
    "#         p2=multi_pdf(X_tensor,mu,cov)\n",
    "#     torch_normal_time.append(time.time() - start_time)\n",
    "\n",
    "#     # torch - GPU\n",
    "    start_time = time.time()\n",
    "    for mu, cov in zip(mus_tensor,covs_tensor):\n",
    "        p2=multi_pdf(X_tensor,mu,cov)\n",
    "    torch_cuda_time.append(time.time() - start_time)\n",
    "    \n",
    "    # torch -GPU Batch\n",
    "    start_time = time.time()\n",
    "    p3 = torch.exp(-0.5*((X_batch-mus_batch)**2 @ torch.linalg.inv(covs_batch)).sum(2))/torch.sqrt((2*np.pi)**2*torch.linalg.det(covs_batch)).view(-1,1)\n",
    "    torch_cuda_batch_time.append(time.time() - start_time)\n",
    "    \n",
    "    \n",
    "print(f\"cpu_normal_time = \\t{np.mean(cpu_normal_time):.4f}\"+u\" \\u00B1 \"+f\"{np.std(cpu_normal_time):.4f} s\")\n",
    "print(f\"cpu_parallel_time = \\t{np.mean(cpu_parallel_time):.4f}\"+u\" \\u00B1 \"+f\"{np.std(cpu_parallel_time):.4f} s\")\n",
    "print(f\"torch_normal_time = \\t{np.mean(torch_normal_time):.4f}\"+u\" \\u00B1 \"+f\"{np.std(torch_normal_time):.4f} s\")\n",
    "print(f\"torch_cuda_time = \\t{np.mean(torch_cuda_time):.4f}\"+u\" \\u00B1 \"+f\"{np.std(torch_cuda_time):.4f} s\")\n",
    "print(f\"torch_cuda_time = \\t{np.mean(torch_cuda_batch_time):.4f}\"+u\" \\u00B1 \"+f\"{np.std(torch_cuda_batch_time):.4f} s\")\n",
    "\n",
    "error = np.linalg.norm(p1-p2.cpu().numpy())\n",
    "error2 = np.linalg.norm(p1-p3.cpu().numpy()[-1,:])\n",
    "print(f\"check error -> {error:.4e}\")\n",
    "print(f\"check error2 -> {error2:.4e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3D data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 30\n",
    "X = np.random.rand(1000,3)\n",
    "mus = np.random.rand(n_components,3)\n",
    "covs = np.array([[1,0,0.1],[0,1,0.2],[0.1,0.2,0.3]])\n",
    "covs = np.tile(covs,(n_components,1,1)).astype(np.float64)\n",
    "X_tensor = torch.from_numpy(X)\n",
    "mus_tensor = torch.from_numpy(mus)\n",
    "covs_tensor = torch.from_numpy(covs)\n",
    "# def fun(X_mu_cov):\n",
    "#     X,mu,cov = X_mu_cov\n",
    "#     distribution = multivariate_normal(mean=mu, cov=cov,allow_singular=True)\n",
    "#     distribution.pdf(X)\n",
    "\n",
    "# num_workers = os.cpu_count()\n",
    "# X_tensor = torch.from_numpy(X)\n",
    "# mus_tensor = torch.from_numpy(mus)\n",
    "# covs_tensor = torch.from_numpy(covs)\n",
    "# def multi_pdf(X,mu,cov):\n",
    "#     N,k = X.size()\n",
    "#     v = X-mu\n",
    "#     x_temp = -0.5* ((X-mu) @ torch.linalg.inv(cov)*(X-mu)).sum(1) # N-by-3 matrix\n",
    "#     numerator = torch.exp(x_temp)\n",
    "#     denomitor = torch.sqrt((2*np.pi)**k*torch.linalg.det(cov))\n",
    "#     return numerator/denomitor\n",
    "    \n",
    "cpu_normal_time = [];cpu_parallel_time=[];torch_normal_time=[];torch_cuda_time=[]\n",
    "\n",
    "for loop in range(10):\n",
    "    # CPU normal\n",
    "    start_time = time.time()\n",
    "    for mu, cov in zip(mus,covs):\n",
    "        distribution = multivariate_normal(mean=mu, cov=cov,allow_singular=True)\n",
    "        p1=distribution.pdf(X)\n",
    "    cpu_normal_time.append(time.time() - start_time)\n",
    "\n",
    "    # CPU parallel\n",
    "    start_time = time.time()\n",
    "    with Pool(num_workers) as p:\n",
    "        p.map(fun,zip(X,mus,covs))\n",
    "    cpu_parallel_time.append(time.time() - start_time)\n",
    "\n",
    "\n",
    "    # torch - CPUtensor([7.0437e-108, 1.0520e-209], device='cuda:0', dtype=torch.float64)\n",
    "    start_time = time.time()\n",
    "    for mu, cov in zip(mus_tensor,covs_tensor):\n",
    "        p2=multi_pdf(X_tensor,mu,cov,k=3)\n",
    "    torch_normal_time.append(time.time() - start_time)\n",
    "\n",
    "    # torch - GPU\n",
    "    start_time = time.time()\n",
    "    for mu, cov in zip(mus_tensor,covs_tensor):\n",
    "        p2=multi_pdf(X_tensor,mu,cov,k=3)\n",
    "    torch_cuda_time.append(time.time() - start_time)\n",
    "\n",
    "print(f\"cpu_normal_time = \\t{np.mean(cpu_normal_time):.4f}\"+u\" \\u00B1 \"+f\"{np.std(cpu_normal_time):.4f} s\")\n",
    "print(f\"cpu_parallel_time = \\t{np.mean(cpu_parallel_time):.4f}\"+u\" \\u00B1 \"+f\"{np.std(cpu_parallel_time):.4f} s\")\n",
    "print(f\"torch_normal_time = \\t{np.mean(torch_normal_time):.4f}\"+u\" \\u00B1 \"+f\"{np.std(torch_normal_time):.4f} s\")\n",
    "print(f\"torch_cuda_time = \\t{np.mean(torch_cuda_time):.4f}\"+u\" \\u00B1 \"+f\"{np.std(torch_cuda_time):.4f} s\")\n",
    "\n",
    "error = np.linalg.norm(p1-p2.cpu().numpy())\n",
    "print(f\"check error -> {error:.4e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1080*720"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "120*80/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
